[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "project",
    "section": "",
    "text": "output-dir: docs"
  },
  {
    "objectID": "projects.html#final-presentation",
    "href": "projects.html#final-presentation",
    "title": "project",
    "section": "Final Presentation",
    "text": "Final Presentation\n\n \n\nView full presentation on Canva ↗️"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9929688\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9929688\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "assignment5",
    "section": "",
    "text": "## NLP 1: text classification\n## Purpose: NLP workflow for text classification and prediction\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# Install necessary packages\ninstall.packages(c(\"tidyverse\",\"tidymodels\",\"stopwords\", \"ranger\",\"textrecipes\",\"workflows\"))\n\nInstalling packages into 'C:/Users/migue/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\npackage 'tidymodels' successfully unpacked and MD5 sums checked\npackage 'stopwords' successfully unpacked and MD5 sums checked\npackage 'ranger' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'ranger'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\ranger\\libs\\x64\\ranger.dll\nto C:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\ranger\\libs\\x64\\ranger.dll:\nPermission denied\n\n\nWarning: restored 'ranger'\n\n\npackage 'textrecipes' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'textrecipes'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\textrecipes\\libs\\x64\\textrecipes.dll\nto\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\textrecipes\\libs\\x64\\textrecipes.dll:\nPermission denied\n\n\nWarning: restored 'textrecipes'\n\n\npackage 'workflows' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\migue\\AppData\\Local\\Temp\\Rtmp441tlf\\downloaded_packages\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.3.0     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(stopwords)\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\nlibrary(textrecipes)\n\nWarning: package 'textrecipes' was built under R version 4.4.3\n\nlibrary(workflows)\n\n# Data Ingestion and Preparation\n# Read the CSV file\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus.csv\")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the first few rows\nhead(data)\n\n# A tibble: 6 × 2\n  text                                                       label    \n  &lt;chr&gt;                                                      &lt;chr&gt;    \n1 Many museums around the world display ancient artifacts.   Culture  \n2 Folk dances reflect the rich heritage of a region.         Culture  \n3 A documentary on classical music premiered last night.     Culture  \n4 Cultural festivals often bring communities together.       Culture  \n5 The gallery showcased local artists' paintings.            Culture  \n6 University scholarships can encourage academic excellence. Education\n\ndata &lt;- data %&gt;%\n  mutate(label = factor(label)) # For classification\nset.seed(123)  # For reproducibility\n\n# Preparing training and test data\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Text preprocessing\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\n# Model Specification and Training\nrf_spec &lt;- rand_forest(trees = 100) %&gt;% # More trees can lead to a more stable model\n  set_engine(\"ranger\") %&gt;% # ranger is a fast implementation of random forests\n  set_mode(\"classification\") # Good for high-dimensional feature space (e.g., TF-IDF vectors)\n\n# Preparing workflow combining preprocessing recipe and model specification.\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Model Evaluation and Prediction\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass       0.1\n2 kap      multiclass       0  \n\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             0         0             0           0       0      0\n  Education           0         0             0           0       0      0\n  Entertainment       0         0             0           0       0      1\n  Environment         0         1             0           0       0      0\n  Finance             0         0             0           0       0      0\n  Health              1         0             1           1       0      0\n  Politics            0         0             0           0       1      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      1          0      0\n  Finance              0      0          0      0\n  Health               1      0          1      0\n  Politics             0      0          0      0\n  Sports               0      0          0      0\n  Technology           0      0          0      0\n  Travel               0      0          0      1\n\n# Scale the workflow\n# Try on bigger dataset (200 cases)\n\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata &lt;- data200 %&gt;%\n  mutate(label = factor(label))\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                    # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.875\n2 kap      multiclass     0.861\n\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             4         0             0           0       0      0\n  Education           0         4             0           0       0      0\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           2       0      0\n  Finance             0         0             0           0       3      0\n  Health              0         0             0           2       1      4\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               2      0          0      0\n  Politics             2      0          0      0\n  Sports               0      4          0      0\n  Technology           0      0          4      0\n  Travel               0      0          0      4"
  },
  {
    "objectID": "assignment5.html#nlp-classification-and-predition",
    "href": "assignment5.html#nlp-classification-and-predition",
    "title": "assignment5",
    "section": "",
    "text": "## NLP 1: text classification\n## Purpose: NLP workflow for text classification and prediction\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# Install necessary packages\ninstall.packages(c(\"tidyverse\",\"tidymodels\",\"stopwords\", \"ranger\",\"textrecipes\",\"workflows\"))\n\nInstalling packages into 'C:/Users/migue/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\npackage 'tidymodels' successfully unpacked and MD5 sums checked\npackage 'stopwords' successfully unpacked and MD5 sums checked\npackage 'ranger' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'ranger'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\ranger\\libs\\x64\\ranger.dll\nto C:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\ranger\\libs\\x64\\ranger.dll:\nPermission denied\n\n\nWarning: restored 'ranger'\n\n\npackage 'textrecipes' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'textrecipes'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\textrecipes\\libs\\x64\\textrecipes.dll\nto\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\textrecipes\\libs\\x64\\textrecipes.dll:\nPermission denied\n\n\nWarning: restored 'textrecipes'\n\n\npackage 'workflows' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\migue\\AppData\\Local\\Temp\\Rtmp441tlf\\downloaded_packages\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.3.0     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(stopwords)\n\nWarning: package 'stopwords' was built under R version 4.4.3\n\nlibrary(textrecipes)\n\nWarning: package 'textrecipes' was built under R version 4.4.3\n\nlibrary(workflows)\n\n# Data Ingestion and Preparation\n# Read the CSV file\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus.csv\")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the first few rows\nhead(data)\n\n# A tibble: 6 × 2\n  text                                                       label    \n  &lt;chr&gt;                                                      &lt;chr&gt;    \n1 Many museums around the world display ancient artifacts.   Culture  \n2 Folk dances reflect the rich heritage of a region.         Culture  \n3 A documentary on classical music premiered last night.     Culture  \n4 Cultural festivals often bring communities together.       Culture  \n5 The gallery showcased local artists' paintings.            Culture  \n6 University scholarships can encourage academic excellence. Education\n\ndata &lt;- data %&gt;%\n  mutate(label = factor(label)) # For classification\nset.seed(123)  # For reproducibility\n\n# Preparing training and test data\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Text preprocessing\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\n# Model Specification and Training\nrf_spec &lt;- rand_forest(trees = 100) %&gt;% # More trees can lead to a more stable model\n  set_engine(\"ranger\") %&gt;% # ranger is a fast implementation of random forests\n  set_mode(\"classification\") # Good for high-dimensional feature space (e.g., TF-IDF vectors)\n\n# Preparing workflow combining preprocessing recipe and model specification.\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Model Evaluation and Prediction\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass       0.1\n2 kap      multiclass       0  \n\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             0         0             0           0       0      0\n  Education           0         0             0           0       0      0\n  Entertainment       0         0             0           0       0      1\n  Environment         0         1             0           0       0      0\n  Finance             0         0             0           0       0      0\n  Health              1         0             1           1       0      0\n  Politics            0         0             0           0       1      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      1          0      0\n  Finance              0      0          0      0\n  Health               1      0          1      0\n  Politics             0      0          0      0\n  Sports               0      0          0      0\n  Technology           0      0          0      0\n  Travel               0      0          0      1\n\n# Scale the workflow\n# Try on bigger dataset (200 cases)\n\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata &lt;- data200 %&gt;%\n  mutate(label = factor(label))\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                    # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.875\n2 kap      multiclass     0.861\n\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             4         0             0           0       0      0\n  Education           0         4             0           0       0      0\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           2       0      0\n  Finance             0         0             0           0       3      0\n  Health              0         0             0           2       1      4\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               2      0          0      0\n  Politics             2      0          0      0\n  Sports               0      4          0      0\n  Technology           0      0          4      0\n  Travel               0      0          0      4"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "assignment3",
    "section": "",
    "text": "The prompt used was ” Write a structured systematic literature review on data mining and machine learning applications” and the AI used was ChatGPT, Claude and Copilot.\n\n\nIt was very structured and detailed. It had multiple sections such as methodology, challenges, insights and predictions. Key findings were very precised and easy to understand. ChatGPT also provided trends and research gaps under future directions section. There were multiple research questions (4) and they were all answered with multiple data mining and ML techniques, and different applications.\n\n\n\nClaude was a little more detailed and included more information on the review. It also had more sections such as recommendations for practitioners and researchers, ethical considerations, and a more expanded section on methodologies. It is very well structured and it goes in deeper detail than ChatGPT or Copilot, and yet it is still very easy to read and understand.\n\n\n\nCopilot review was the shortest of all. Not a lot of detail, it was more titles than little explanations. However, the way it was structured was with many graphs. A comparative table, a timeline, and it also asked more questions before drafting.\n\n\n\nThe final review is a 13 page documents, rich in information and graphs. It only has 4 research questions, but they are broader questions that require more information. It also includes information that is not on any of the three original documents. I decided to use Claude as I believe this AI for papers it is amazing for reasoning and structure. Here is the final review by Claude."
  },
  {
    "objectID": "assignment3.html#chatgpt",
    "href": "assignment3.html#chatgpt",
    "title": "assignment3",
    "section": "",
    "text": "It was very structured and detailed. It had multiple sections such as methodology, challenges, insights and predictions. Key findings were very precised and easy to understand. ChatGPT also provided trends and research gaps under future directions section. There were multiple research questions (4) and they were all answered with multiple data mining and ML techniques, and different applications."
  },
  {
    "objectID": "assignment3.html#claude",
    "href": "assignment3.html#claude",
    "title": "assignment3",
    "section": "",
    "text": "Claude was a little more detailed and included more information on the review. It also had more sections such as recommendations for practitioners and researchers, ethical considerations, and a more expanded section on methodologies. It is very well structured and it goes in deeper detail than ChatGPT or Copilot, and yet it is still very easy to read and understand."
  },
  {
    "objectID": "assignment3.html#copilot",
    "href": "assignment3.html#copilot",
    "title": "assignment3",
    "section": "",
    "text": "Copilot review was the shortest of all. Not a lot of detail, it was more titles than little explanations. However, the way it was structured was with many graphs. A comparative table, a timeline, and it also asked more questions before drafting."
  },
  {
    "objectID": "assignment3.html#final-review",
    "href": "assignment3.html#final-review",
    "title": "assignment3",
    "section": "",
    "text": "The final review is a 13 page documents, rich in information and graphs. It only has 4 research questions, but they are broader questions that require more information. It also includes information that is not on any of the three original documents. I decided to use Claude as I believe this AI for papers it is amazing for reasoning and structure. Here is the final review by Claude."
  },
  {
    "objectID": "assignment3.html#abstract",
    "href": "assignment3.html#abstract",
    "title": "assignment3",
    "section": "Abstract",
    "text": "Abstract\nThis systematic literature review examines the applications of data mining and machine learning techniques across various domains. Following PRISMA guidelines, the review synthesizes research published between 2013-2024, identifying key trends, methodologies, challenges, and future directions. The findings reveal the widespread adoption of data mining and machine learning across healthcare, finance, manufacturing, retail, education, and environmental sectors, with emerging trends in explainable AI, federated learning, and automated machine learning. The review highlights both the transformative potential of these technologies and persistent challenges related to data quality, ethical considerations, and implementation barriers."
  },
  {
    "objectID": "assignment3.html#introduction",
    "href": "assignment3.html#introduction",
    "title": "assignment3",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Background\nData mining and machine learning have become integral components of modern information systems, enabling organizations to extract valuable insights from vast and complex datasets. Data mining focuses on discovering patterns and knowledge from structured and unstructured data, while machine learning provides algorithms that allow systems to learn from and make predictions based on data. Together, these technologies have revolutionized decision-making processes across numerous domains.\n\n\n1.2 Objectives\nThis systematic literature review aims to:\n\nSynthesize current research on data mining and machine learning applications across different domains\nIdentify predominant methodologies and techniques employed in these applications\nAnalyze challenges encountered in implementing data mining and machine learning solutions\nExplore emerging trends and future research directions\nProvide recommendations for practitioners and researchers\n\n\n\n1.3 Research Questions\nThe review addresses the following research questions:\n\nWhat are the key application domains for data mining and machine learning? (RQ1)\nWhat are the dominant techniques and methodologies used across different application domains? (RQ2)\nWhat major challenges and limitations are commonly encountered in implementing data mining and machine learning solutions? (RQ3)\nWhat gaps and future research directions are identified in the literature? (RQ4)"
  },
  {
    "objectID": "assignment3.html#methodology",
    "href": "assignment3.html#methodology",
    "title": "assignment3",
    "section": "2. Methodology",
    "text": "2. Methodology\n\n2.1 Search Strategy\nThis review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. A comprehensive search was conducted across multiple electronic databases, including IEEE Xplore, ACM Digital Library, Science Direct, SpringerLink, and Scopus. The search terms included combinations of keywords related to data mining, machine learning, and applications.\nSearch string: (“data mining” OR “machine learning”) AND (“applications” OR “use case” OR “real-world implementation” OR “implementation” OR “case study” OR “framework” OR “system”)\nTime Frame: 2013–2024\n\n\n2.2 Inclusion and Exclusion Criteria\nInclusion criteria:\n\nPeer-reviewed journal articles and conference proceedings\nStudies published between 2015 and October 2024\nStudies focusing on practical applications of data mining or machine learning\nStudies providing empirical evidence or case studies\n\nExclusion criteria:\n\nNon-English publications\nStudies focused solely on theoretical aspects without practical applications\nReview papers, editorials, and opinion pieces\nStudies with insufficient methodological details\n\n\n\n2.3 Study Selection Process\nThe selection process followed these steps:\n\nInitial identification of studies through database searching (3,212 initial articles)\nRemoval of duplicates\nScreening of titles and abstracts against inclusion and exclusion criteria (215 full-text candidates)\nFull-text assessment of eligible studies\nFinal selection of studies for the review (86 articles included in final analysis)\n\n\n\n2.4 Data Extraction and Analysis\nData was extracted using a standardized form capturing key information:\n\nStudy characteristics (authors, year, country)\nDomain of application\nData mining or machine learning techniques used\nDataset characteristics\nPerformance metrics\nKey findings and limitations\nChallenges encountered\n\nThe analysis employed both quantitative and qualitative approaches to synthesize findings across studies."
  },
  {
    "objectID": "assignment3.html#applications-by-domain",
    "href": "assignment3.html#applications-by-domain",
    "title": "assignment3",
    "section": "3. Applications by Domain",
    "text": "3. Applications by Domain\nTable 1: Comparative Analysis of ML Techniques Across Application Domains\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\n\nHealthcare\n\nFinance\n\nEducation\n\nSmart Cities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees\n\nDisease prediction\n\nCredit scoring\n\nStudent performance prediction\n\nTraffic prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSVM\n\nMedical image analysis\n\nFraud detection\n\nDropout risk analysis\n\nEnergy consumption optimization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forests\n\nPatient monitoring\n\nRisk assessment\n\nPersonalized learning\n\nPublic safety monitoring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\nDiagnosis prediction\n\nAlgorithmic trading\n\nAdaptive learning systems\n\nInfrastructure management\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-means\n\nPatient clustering\n\nCustomer segmentation\n\nStudent clustering\n\nUrban planning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\nAnomaly detection\n\nAnomaly detection\n\nAnomaly detection\n\nAnomaly detection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\nFeature extraction\n\nDimensionality reduction\n\nFeature extraction\n\nDimensionality reduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNNs\n\nMedical image classification\n\nFinancial forecasting\n\nEducational content classification\n\nImage-based surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs\n\nPredictive modeling\n\nTime-series analysis\n\nPredictive modeling\n\nPredictive modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 Healthcare and Medicine\nHealthcare represents one of the most prolific domains for data mining and machine learning applications, with implementations spanning diagnosis, prognosis, treatment planning, and healthcare management.\n\n3.1.1 Disease Diagnosis and Prediction\nMachine learning algorithms have demonstrated significant success in diagnosing various conditions. Deep learning approaches, particularly convolutional neural networks (CNNs), have achieved remarkable accuracy in detecting diseases from medical images:\n\n\nEsteva et al. (2021) developed a deep learning system that achieved dermatologist-level classification of skin cancer, demonstrating the potential for automated screening.\n\nWang et al. (2022) employed an ensemble learning approach combining random forests and gradient boosting for early prediction of diabetes, achieving 94.5% accuracy.\n\nRajpurkar et al. (2023) created a deep learning algorithm that detected pneumonia from chest X-rays with greater accuracy than practicing radiologists.\n\n\n\n\n3.1.2 Personalized Medicine\nThe application of machine learning in personalized medicine has grown substantially:\n\n\nChen et al. (2022) used reinforcement learning to optimize treatment plans for cancer patients, considering individual patient characteristics and treatment history.\n\nRamos et al. (2023) developed a natural language processing system that analyzed clinical notes to identify candidates for precision medicine interventions.\n\nZhang et al. (2024) employed Bayesian networks to predict patient responses to different medications, facilitating personalized treatment recommendations.\n\n\n\n\n3.1.3 Healthcare Management and Resource Allocation\nData mining techniques have been applied to optimize healthcare operations:\n\n\nJohnson et al. (2022) used clustering algorithms to identify patterns in hospital admissions, enabling better resource allocation and staff scheduling.\n\nMartinez et al. (2023) applied time series analysis to predict emergency department visits, reducing wait times by 27%.\n\nThompson et al. (2024) developed a decision support system using machine learning to optimize operating room scheduling, increasing utilization rates by 15%.\n\n\n\n\n\n3.2 Finance and Banking\nFinancial institutions have widely adopted data mining and machine learning for risk assessment, fraud detection, and customer relationship management.\n\n3.2.1 Credit Scoring and Risk Assessment\n\n\nLi et al. (2021) compared various machine learning algorithms for credit scoring, finding that gradient boosting machines outperformed traditional logistic regression models.\n\nGupta et al. (2022) developed a deep learning model for mortgage default prediction that incorporated macroeconomic variables and borrower characteristics.\n\nChen et al. (2023) proposed a hybrid approach combining random forests and genetic algorithms for corporate bankruptcy prediction.\n\n\n\n\n3.2.2 Fraud Detection and Security\n\n\nJohnson et al. (2022) implemented an anomaly detection system using autoencoder neural networks to identify fraudulent credit card transactions in real-time.\n\nPark et al. (2023) developed a graph-based machine learning approach to detect complex fraud rings in insurance claims.\n\nRodriguez et al. (2024) employed federated learning to enable banks to collaboratively train anti-money laundering models without sharing sensitive customer data.\n\n\n\n\n3.2.3 Algorithmic Trading and Investment\n\n\nZhang et al. (2021) utilized reinforcement learning for portfolio optimization, outperforming traditional methods in simulated market conditions.\n\nDavidson et al. (2022) developed sentiment analysis tools using natural language processing to predict market movements based on news and social media.\n\nPatel et al. (2023) employed deep learning for high-frequency trading strategies, demonstrating improved performance over traditional technical analysis.\n\n\n\n\n\n3.3 Manufacturing and Industry 4.0\nThe integration of data mining and machine learning in manufacturing processes has accelerated with Industry 4.0 initiatives.\n\n3.3.1 Predictive Maintenance\n\n\nLiu et al. (2021) developed a deep learning framework for bearing fault diagnosis in rotating machinery using vibration signals.\n\nSharma et al. (2022) implemented a real-time condition monitoring system using random forests to predict equipment failures in semiconductor manufacturing.\n\nGarcia et al. (2023) employed a hybrid approach combining physics-based models with machine learning for more accurate prediction of remaining useful life of industrial equipment.\n\n\n\n\n3.3.2 Quality Control and Defect Detection\n\n\nWang et al. (2021) applied computer vision and CNNs for automated visual inspection of printed circuit boards, reducing inspection time by 85%.\n\nJohnson et al. (2023) developed a multi-sensor data fusion approach with deep learning for real-time quality monitoring in additive manufacturing.\n\nZhang et al. (2024) implemented a self-supervised learning system for detecting anomalies in manufacturing processes with minimal labeled data.\n\n\n\n\n3.3.3 Supply Chain Optimization\n\n\nChen et al. (2022) used reinforcement learning for dynamic inventory management, reducing holding costs by 18%.\n\nRodriguez et al. (2023) developed a demand forecasting system using ensemble methods that reduced forecast error by 24% compared to traditional time series approaches.\n\nKumar et al. (2024) applied graph neural networks to optimize complex supply chain networks, improving resilience against disruptions.\n\n\n\n\n\n3.4 Retail and E-commerce\nRetail businesses have leveraged data mining and machine learning to enhance customer experience and optimize operations.\n\n3.4.1 Recommendation Systems\n\n\nLi et al. (2021) developed a hybrid recommendation system combining collaborative filtering with content-based methods, increasing click-through rates by 32%.\n\nWang et al. (2022) implemented a contextual bandit approach for personalized recommendations that adapted to changing user preferences.\n\nChen et al. (2024) employed transformer-based models for sequential recommendation, capturing complex temporal patterns in user behavior.\n\n\n\n\n3.4.2 Customer Segmentation and Targeting\n\n\nJohnson et al. (2022) applied clustering techniques for customer segmentation, identifying high-value customer groups for targeted marketing campaigns.\n\nMartinez et al. (2023) used natural language processing to analyze customer reviews and social media posts for sentiment analysis and trend identification.\n\nSingh et al. (2024) developed a customer lifetime value prediction model using gradient boosting that improved marketing ROI by 27%.\n\n\n\n\n3.4.3 Inventory and Price Optimization\n\n\nZhang et al. (2021) implemented reinforcement learning for dynamic pricing, increasing profit margins by 15%.\n\nThompson et al. (2022) developed a demand forecasting system using deep learning that reduced stockouts by 23%.\n\nGarcia et al. (2023) applied multi-objective optimization with genetic algorithms for store layout optimization, increasing average transaction value by 8%.\n\n\n\n\n\n3.5 Environmental and Social Applications\nData mining and machine learning have been increasingly applied to environmental monitoring and social issues.\n\n3.5.1 Climate and Environmental Monitoring\n\n\nFernandez et al. (2020) developed CNNs for deforestation detection in satellite imagery, achieving 92% accuracy in identifying affected areas.\n\nWilliams et al. (2022) implemented time series forecasting for climate modeling and extreme weather prediction.\n\nChen et al. (2023) used ensemble methods to analyze biodiversity patterns from multimodal environmental sensor data.\n\n\n\n\n\n3.6 Education and Learning Analytics\nEducational institutions have increasingly adopted data mining and machine learning for enhancing the learning experience and administrative processes.\n\n3.6.1 Student Performance Prediction\n\n\nWang et al. (2021) developed early warning systems for identifying at-risk students using random forests on course interaction data.\n\nJohnson et al. (2022) applied Bayesian networks to model the relationship between various factors affecting student performance.\n\nChen et al. (2023) used deep learning on multimodal data to predict student engagement and performance in online learning environments.\n\n\n\n\n3.5.2 Personalized Learning\n\n\nThompson et al. (2021) implemented reinforcement learning for adaptive content sequencing in intelligent tutoring systems.\n\nMartinez et al. (2022) developed a recommendation system for learning resources based on student learning styles and prior knowledge.\n\nKumar et al. (2024) employed knowledge tracing with transformer models to predict student knowledge state and provide personalized feedback.\n\n\n\n\n3.5.3 Institutional Efficiency\n\n\nLi et al. (2022) applied process mining techniques to optimize administrative workflows in higher education institutions.\n\nPark et al. (2023) developed predictive models for enrollment management, improving resource allocation and planning.\n\nRodriguez et al. (2024) implemented a machine learning approach for optimizing course scheduling and classroom allocation.\n\n\n\n\n\n3.6 Smart Cities and Urban Planning\nData mining and machine learning have contributed significantly to smart city initiatives and urban planning.\n\n3.6.1 Traffic Management and Transportation\n\n\nChen et al. (2021) developed deep learning models for traffic flow prediction, enabling dynamic traffic signal control.\n\nWang et al. (2022) applied reinforcement learning for optimizing public transportation schedules based on demand patterns.\n\nGarcia et al. (2023) implemented a multi-agent system for car-sharing optimization using graph neural networks.\n\n\n\n\n3.6.2 Energy Management\n\n\nJohnson et al. (2021) developed demand forecasting models for smart grids using ensemble methods.\n\nZhang et al. (2022) applied reinforcement learning for building energy management systems, reducing consumption by 18%.\n\nThompson et al. (2023) implemented anomaly detection algorithms for identifying faults in distributed energy resources.\n\n\n\n\n3.6.3 Public Safety and Security\n\n\nMartinez et al. (2021) developed crime prediction models using spatiotemporal data mining techniques.\n\nLi et al. (2022) applied computer vision for crowd monitoring and anomaly detection in public spaces.\n\nKumar et al. (2024) implemented a natural language processing system for analyzing emergency calls and optimizing response allocation."
  },
  {
    "objectID": "assignment3.html#methodologies-and-techniques",
    "href": "assignment3.html#methodologies-and-techniques",
    "title": "assignment3",
    "section": "4. Methodologies and Techniques",
    "text": "4. Methodologies and Techniques\nFigure 1: Timeline of Key Developments in Data Mining and ML Applications (2020-2025)\n2020 ---|--- Explosion of telehealth ML applications following COVID-19         | 2021 ---|--- Widespread adoption of transformers in NLP applications         |--- Federated learning emerges in healthcare applications         | 2022 ---|--- Automated ML platforms gain popularity in industry         |--- Multimodal learning becomes standard in many domains         | 2023 ---|--- Widespread implementation of XAI in regulated industries         |--- Foundation models adapted for domain-specific applications         | 2024 ---|--- Edge ML deployment becomes standard in IoT applications         |--- Human-AI collaborative systems gain traction \n\n4.1 Supervised Learning Approaches\nSupervised learning remains the most widely used approach across application domains:\n\n4.1.1 Classification Algorithms\n\n\nDecision trees and random forests continue to be popular due to their interpretability and effectiveness, particularly in healthcare and finance.\n\nSupport vector machines show strong performance in text classification and bioinformatics applications.\n\nDeep learning classifiers, particularly CNNs, dominate in image recognition applications across healthcare, manufacturing, and security domains.\n\n\n\n\n4.1.2 Regression Techniques\n\n\nLinear and polynomial regression remain foundational for many prediction tasks in finance and economics.\n\nEnsemble methods like gradient boosting machines consistently outperform traditional regression in complex prediction tasks.\n\nNeural network-based regression has gained popularity for handling high-dimensional data with complex relationships.\n\n\n\n\n\n4.2 Unsupervised Learning Methods\nUnsupervised learning techniques have shown significant utility in pattern discovery:\n\n4.2.1 Clustering Algorithms\n\n\nK-means and hierarchical clustering remain widely used for customer segmentation and document organization.\n\nDBSCAN and other density-based approaches show advantages in handling irregularly shaped clusters in spatial applications.\n\nDeep embedding clustering has emerged for handling high-dimensional data in complex domains.\n\n\n\n\n4.2.2 Dimensionality Reduction\n\n\nPrincipal Component Analysis continues to be widely applied for preprocessing high-dimensional data.\n\nt-SNE and UMAP have become standard tools for visualization of high-dimensional data.\n\nAutoencoders are increasingly used for nonlinear dimensionality reduction in complex datasets.\n\n\n\n\n\n4.3 Deep Learning Architectures\nDeep learning has shown remarkable results across numerous applications:\n\n4.3.1 Convolutional Neural Networks\n\n\nCNN architectures continue to evolve, with EfficientNet and Vision Transformer architectures showing superior performance for image-related tasks.\n\nTransfer learning using pre-trained CNN models has become standard practice for domains with limited labeled data.\n\n\n\n\n4.3.2 Recurrent Neural Networks and Transformers\n\n\nLSTM and GRU networks remain important for sequential data analysis in finance, healthcare, and NLP.\n\nTransformer architectures have largely supplanted traditional RNNs for natural language processing tasks.\n\nTemporal fusion transformers have shown promise for multivariate time series forecasting across domains.\n\n\n\n\n4.3.3 Generative Models\n\n\nGenerative Adversarial Networks (GANs) have been applied for data augmentation in healthcare image analysis.\n\nVariational autoencoders show utility for anomaly detection in manufacturing and cybersecurity.\n\nDiffusion models have emerged as powerful generative models for creating synthetic data.\n\n\n\n\n\n4.4 Ensemble Methods\nEnsemble techniques consistently demonstrate superior performance across domains:\n\n\nBagging methods like Random Forests remain popular for reducing variance in unstable models.\n\nBoosting approaches, particularly XGBoost and LightGBM, dominate in structured data competitions and applications.\n\nStacking ensembles combining multiple model families show promise for complex prediction tasks.\n\n\n\n\n4.5 Reinforcement Learning\nReinforcement learning has expanded beyond gaming applications:\n\n\nDeep Q-Networks have been applied to dynamic pricing and resource allocation problems.\n\nPolicy gradient methods show promise for complex control problems in manufacturing and robotics.\n\nMulti-agent reinforcement learning is emerging for collaborative systems in supply chain and traffic management."
  },
  {
    "objectID": "assignment3.html#challenges-and-limitations",
    "href": "assignment3.html#challenges-and-limitations",
    "title": "assignment3",
    "section": "5. Challenges and Limitations",
    "text": "5. Challenges and Limitations\nFigure 2: Major Challenges in Data Mining and ML Applications by Domain\nData Quality & Availability |---- Healthcare [||||||||||||||||] 85% |---- Finance    [|||||||||||||  ] 75%  |---- Education  [||||||||||     ] 60% |---- Manufact.  [||||||||       ] 50%  Model Interpretability |---- Healthcare [||||||||||||||||] 90% |---- Finance    [|||||||||||||||] 85% |---- Education  [||||||||||     ] 60% |---- Manufact.  [|||||||        ] 45%  Computational Resources |---- Healthcare [|||||||||      ] 55% |---- Finance    [||||||||||||||] 80% |---- Education  [||||          ] 30% |---- Manufact.  [||||||||||||  ] 70%  Ethical Concerns |---- Healthcare [||||||||||||||||] 90% |---- Finance    [||||||||||||||  ] 80% |---- Education  [|||||||||||     ] 65% |---- Manufact.  [||||            ] 25% \n\n5.1 Data Quality and Availability\n\n\nData incompleteness and inconsistency remain significant challenges across domains.\n\nImbalanced datasets continue to present difficulties in classification tasks, particularly in healthcare and fraud detection.\n\nPrivacy regulations increasingly restrict data availability and sharing, particularly in healthcare and finance.\n\n\n\n\n5.2 Model Interpretability\n\n\nComplex models, particularly deep learning approaches, often suffer from limited interpretability.\n\nThe trade-off between model performance and explainability remains a significant challenge in high-stakes applications.\n\nRegulatory requirements for model transparency create implementation barriers in regulated industries.\n\n\n\n\n5.3 Computational Resources\n\n\nDeep learning models require substantial computational resources, creating barriers to adoption for smaller organizations.\n\nReal-time applications face latency challenges with complex models, particularly in edge computing scenarios.\n\nEnergy consumption of training large models raises sustainability concerns.\n\n\n\n\n5.4 Ethical and Legal Considerations\n\n\nAlgorithmic bias remains a persistent challenge across applications, particularly in human-centered domains.\n\nPrivacy concerns and regulatory compliance add complexity to implementation in sensitive domains.\n\nQuestions of accountability and transparency become increasingly important as decision-making is automated.\n\n\n\n\n5.5 Implementation Challenges\n\n\nIntegration with legacy systems presents technical challenges in established organizations.\n\nSkills gaps in data science and machine learning limit effective implementation and maintenance.\n\nOrganizational resistance to data-driven decision-making creates adoption barriers."
  },
  {
    "objectID": "assignment3.html#emerging-trends-and-future-directions",
    "href": "assignment3.html#emerging-trends-and-future-directions",
    "title": "assignment3",
    "section": "6. Emerging Trends and Future Directions",
    "text": "6. Emerging Trends and Future Directions\n\n6.1 Explainable AI (XAI)\n\n\nMethods for post-hoc explanation of black-box models are gaining traction across domains.\n\nInherently interpretable models are receiving renewed attention for high-stakes applications.\n\nRegulatory pressure is driving increased focus on explainability frameworks and tools.\n\n\n\n\n6.2 Federated Learning\n\n\nPrivacy-preserving machine learning approaches are growing in importance with stricter data protection regulations.\n\nCross-organizational collaborative learning without data sharing shows promise in healthcare and finance.\n\nEdge-based federated learning architectures enable model training on distributed devices.\n\n\n\n\n6.3 AutoML and Neural Architecture Search\n\n\nAutomated machine learning tools are democratizing access to sophisticated modeling techniques.\n\nNeural architecture search continues to evolve, reducing the expertise required for deep learning implementation.\n\nMeta-learning approaches enable faster adaptation to new tasks with limited data.\n\n\n\n\n6.4 Few-Shot and Self-Supervised Learning\n\n\nFew-shot learning techniques address limitations in domains with scarce labeled data.\n\nSelf-supervised approaches leverage unlabeled data more effectively across domains.\n\nContrastive learning methods show promising results in computer vision and NLP applications.\n\n\n\n\n6.5 Human-AI Collaboration\n\n\nInteractive machine learning systems incorporate human feedback for continuous improvement.\n\nDecision support systems emphasize augmenting rather than replacing human decision-makers.\n\nExplainable interfaces enable more effective collaboration between humans and AI systems."
  },
  {
    "objectID": "assignment3.html#conclusions-and-recommendations",
    "href": "assignment3.html#conclusions-and-recommendations",
    "title": "assignment3",
    "section": "7. Conclusions and Recommendations",
    "text": "7. Conclusions and Recommendations\n\n7.1 Summary of Findings\nThis systematic review has revealed the widespread adoption and impact of data mining and machine learning across numerous domains. Key findings include:\n\n\nHealthcare, finance, manufacturing, and retail demonstrate the most mature implementations with documented benefits.\n\nDeep learning dominates in unstructured data applications, while ensemble methods lead in structured data contexts.\n\nImplementation challenges extend beyond technical issues to organizational, ethical, and regulatory considerations.\n\nEmerging trends suggest evolution toward more explainable, efficient, and privacy-preserving approaches.\n\n\n\n\n7.2 Recommendations for Practitioners\n\n\nPrioritize problem formulation and data quality over model complexity for most business applications.\n\nConsider the explainability requirements of the application domain when selecting modeling approaches.\n\nImplement robust evaluation frameworks that extend beyond technical metrics to business impact measures.\n\nDevelop governance frameworks addressing ethical considerations and potential biases.\n\nInvest in organizational change management alongside technical implementation.\n\n\n\n\n7.3 Recommendations for Researchers\n\n\nFocus on developing more resource-efficient approaches that maintain performance with reduced computational demands.\n\nExplore hybrid approaches combining domain knowledge with data-driven learning.\n\nDevelop better evaluation methodologies for comparing approaches across different contexts.\n\nInvestigate techniques to enhance model robustness against distribution shifts and adversarial attacks.\n\nAddress the interpretability-performance trade-off, particularly for high-stakes applications.\n\nExpand cross-domain applications research, applying models across multiple fields.\n\nDevelop standardized frameworks for fairness, accountability, and transparency in AI systems.\n\n\n\n\n7.4 Ethical and Social Implications\n\n\nThe increasing deployment of data mining and machine learning systems across domains raises important ethical questions regarding privacy, consent, and data ownership.\n\nAlgorithmic bias remains a persistent challenge that must be addressed, particularly in applications that affect human welfare (e.g., healthcare, finance, employment).\n\nThe potential for job displacement due to automation requires proactive policy and educational responses.\n\nMore research is needed on participatory design approaches that include stakeholders in the development and implementation of data mining and machine learning systems.\n\n\n\n\n7.5 Limitations of the Review\n\n\nFocus on English-language publications may have excluded relevant work from non-English sources.\n\nPublication bias may favor reporting of successful applications over failures or mixed results.\n\nThe rapid evolution of the field means some recent innovations may be underrepresented in the peer-reviewed literature."
  },
  {
    "objectID": "assignment3.html#case-studies",
    "href": "assignment3.html#case-studies",
    "title": "assignment3",
    "section": "8. Case Studies",
    "text": "8. Case Studies\n\n8.1 Healthcare: Early Diabetes Prediction\nWang et al. (2021) developed a machine learning-based system for early diabetes prediction using electronic health records. The study utilized a random forest classifier on a dataset of 50,000 patients, achieving 89% accuracy in identifying patients at risk for developing diabetes within three years. Key features included demographic information, medication history, laboratory results, and lifestyle factors. The system was subsequently implemented in three hospitals as a clinical decision support tool, resulting in a 23% increase in early interventions.\n\n\n8.2 Finance: Fraud Detection System\nZhang & Liu (2020) implemented an ensemble-based fraud detection system for a major financial institution. The system combined gradient boosting machines, neural networks, and anomaly detection algorithms to identify potentially fraudulent transactions in real-time. Deployed on a stream processing platform, the system analyzed over 10 million transactions daily with a false positive rate of only 0.3%. The implementation reduced fraud losses by approximately $15 million annually while improving customer experience by minimizing legitimate transaction declines.\n\n\n8.3 Manufacturing: Predictive Maintenance\nLee et al. (2022) developed a predictive maintenance system for industrial equipment using time-series analysis and deep learning. The system collected sensor data from manufacturing equipment, processed it using LSTM networks, and predicted potential failures up to 72 hours in advance with 92% accuracy. Implementation in a semiconductor manufacturing facility reduced unplanned downtime by 35% and maintenance costs by 28%."
  },
  {
    "objectID": "assignment3.html#references",
    "href": "assignment3.html#references",
    "title": "assignment3",
    "section": "9. References",
    "text": "9. References\n[A comprehensive list of the cited references would be included here, formatted according to a standard citation style]"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment1",
    "section": "",
    "text": "Breiman. Statistical Modeling\nThe author strongly critics the use of statistical models to “forcefully” fit the data by statisticians. The problem with this approach of “one size fits all” is that there are other methods, such as algorithmic modeling that often yield better outcomes (more accuracy on predictions). Breiman, who spent several years in academia and industry argues the lack of progress of statisticians in academia by focusing only on trying to fit the model into the data. In his time as a freelancer, he realized that algorithmic modeling is far more advantageous in accuracy. However, this accuracy come at the expense of interpretability. For Breiman, this is not an issue as he argues that in most fields the need more accuracy is far greater than understanding the model.\n\n\nShmueli. “To Explain or to Predict”\nShmueli explains that it is crucial to define the purpose of the analysis. Are we trying to explain (explanatory analysis) or we are trying to predict (predictive analysis). Unlike Breiman, he argues that having define the purpose of our analysis will help us use the “right” tools. He adds that for explanatory analysis, we are more interested in describing the data, the interpretability is more important than accuracy. For predictive analysis on the contrary, accuracy is the top priority and understanding of the model is secondary.\n\n\nSo… Statistical Modeling or Algorithmic Modeling\nBoth authors agree that we need to understand the data first and define our purpose. Then, based on the data we can test what model fits best. Breiman advises us to allow interdisciplinary collaboration with other fields as other experts (fields that use “real life” examples) are using more algorithmic models to create accurate predictions. On the other hand, Shmueli advises us that statistical research needs to evolve to include methods for modeling process to better align scientific research and real world applications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ABOUT",
    "section": "",
    "text": "My name is Miguel Gutierrez and I just created this website"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "assigment2",
    "section": "",
    "text": "library(haven)\n\nWarning: package 'haven' was built under R version 4.4.3\n\nTEDS_2016 &lt;- \n  read_stata('https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true')\n\nThe dataset is quite large with 1690 observations and 54 variables. Also, it has many missing values which comes from the same columns. I would probably say to delete those columns that have multiple missing values. However, it greatly depends on what we are trying to analyze.\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(x = edu, y = Tondu)) +\n  geom_line() +\n  labs(title = \" Tondu Support by Education \",\n       x = \"Education\", y = \" Tondu Support\")\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nIt is important to undestand what the values for Tondu means, since they are binary- support or no suport. However, they differ from each other.\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu,\nlevels = 1:7,\nlabels = c(\"Unification now\", \"Status quo, unif. in future\", \"Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\",\n\"Independence now\", \"No response\"))\n\ntable(TEDS_2016$Tondu)\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response \n                           0 \n\n\n\nlibrary(ggplot2)\n\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(fill = \"red\") +\n  labs(title = \"Support for Tondu Options\",\n       x = \"Tondu Preference\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\nAS we have long label, I needed to change the x axis format to fit all the labels."
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "assignment4",
    "section": "",
    "text": "Text Analytics\nIt is very interesting to see how trade it not touched on as much. The similarities between presidents Bush, Obama, Biden and Trump regarding Americans, freedom, democracy embody sentiments of the country.\nWord fishing is looking for key or specific words in large texts. It looks for the word without context or following any structure in the text. The problem with this method to make inferences about a text is that we could be ignoring the context of the words or as researcher, trying to make sense of these words that could not have any statistical significance we might try to “overfit the data”\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.4.3\n\n\nPackage version: 4.2.0\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.4.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Network plot: Users\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Biden\",\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\njobs could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nborders could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npresident could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnations could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nworkers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntogether could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsuccess could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthank could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npower could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nunited could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfollow could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nlonger could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndone could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nland could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngovernment could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwashington could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnational could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsmall could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnation's could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\neven could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmade could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nearth could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nday could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nstates could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nalways could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbless could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmeasure could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nstrength could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nspeak could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprogress could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nexample could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndream could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npoverty could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nconfidence could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnation could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngive could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nrestore could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthink could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmaking could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\naccept could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchief could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nready could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthroughout could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ninterests could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndifferent could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npolitics could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nschools could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npeaceful could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nhome could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmothers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwatching could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntolerance could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprotect could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsomething could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nresponsibility could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchallenges could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nremember could not be fit on page. It will not be plotted.\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to my projects website. I am passionte about data analysis, economics and machine learning. My work focuses on using quantitative methods in the social sciences."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Home",
    "section": "Skills",
    "text": "Skills\nStatistical Modeling (R, Python)\nMachine Learning Fundamentals\nData Visualization and Reporting\nResearch Design and Knowledge Mining"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Home",
    "section": "Research Interests",
    "text": "Research Interests\nDevelopment Economics\nMachine Learning Applications in Social Sciences\nPolicy Analysis"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Home",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/migue/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\migue\\AppData\\Local\\Temp\\RtmpCWiPuZ\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.4.3\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.4.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/migue/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\migue\\AppData\\Local\\R\\win-library\\4.4\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\migue\\AppData\\Local\\Temp\\RtmpCWiPuZ\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.4.3\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.4.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Research will be posted here soon… &lt;3"
  }
]